{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to run Curvefit across all locations for model validation exercise\n",
    "## All models will be run in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "from db_queries import get_location_metadata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "from covid_model_deaths import runner\n",
    "from covid_model_deaths.deaths_io import InputsContext, MEASURES, Checkpoint\n",
    "from covid_model_deaths.globals import COLUMNS\n",
    "\n",
    "pd.options.display.max_rows = 99\n",
    "pd.options.display.max_columns = 99\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set dates for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DATE = \"2020_05_29\" # Date for all data used\n",
    "# Completed these \"2020-03-27\", \"2020-04-03\", \"2020-04-10\",\n",
    "# \"2020-04-17\" breaks, do the rest?\n",
    "date_list = [\"2020-04-24\",\"2020-05-01\",\"2020-05-08\",\"2020-05-15\",\"2020-05-22\",\"2020-05-29\"]\n",
    "date_list = [\"2020-03-27\", \"2020-04-03\", \"2020-04-10\", \"2020-04-17\"]\n",
    "date_list = [\"2020-04-07\",\"2020-04-09\",\"2020-04-11\",\"2020-04-14\",\"2020-04-21\",\"2020-04-28\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-03 13:32:36.554 | DEBUG    | covid_model_deaths.deaths_io.checkpoint:_setup_checkpoint_dir:45 - Making checkpoint directory at /ihme/covid-19/deaths/validation/2020_05_29/2020_04_17/checkpoint\n",
      "2020-06-03 13:32:36.579 | DEBUG    | covid_model_deaths.deaths_io.inputs:load:40 - Loading full_data.csv from /ihme/covid-19/model-inputs/2020_05_29.01.\n",
      "2020-06-03 13:32:36.627 | DEBUG    | covid_model_deaths.deaths_io.inputs:load:40 - Loading deaths.csv from /ihme/covid-19/model-inputs/2020_05_29.01.\n",
      "2020-06-03 13:32:36.665 | DEBUG    | covid_model_deaths.deaths_io.inputs:load:40 - Loading age_pop.csv from /ihme/covid-19/model-inputs/2020_05_29.01.\n",
      "2020-06-03 13:32:36.671 | DEBUG    | covid_model_deaths.deaths_io.inputs:load:40 - Loading age_death.csv from /ihme/covid-19/model-inputs/2020_05_29.01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /ihme/covid-19/deaths/validation/2020_05_29/2020_04_17\n",
      "/ihme/code/covid-19/user/ctroeger/covid-model-deaths/src/covid_model_deaths\n",
      "Checkpoint(/ihme/covid-19/deaths/validation/2020_05_29/2020_04_17/checkpoint)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-03 13:32:49.557 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading full_data from in memory cache.\n",
      "2020-06-03 13:32:49.560 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading deaths from in memory cache.\n",
      "2020-06-03 13:32:49.561 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading age_pop from in memory cache.\n",
      "2020-06-03 13:32:49.563 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading age_death from in memory cache.\n",
      "100%|██████████| 313/313 [02:51<00:00,  1.82it/s]\n",
      "2020-06-03 13:35:42.013 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading cases_and_backcast_deaths from in memory cache.\n",
      "2020-06-03 13:35:42.015 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading location from in memory cache.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-25b3edf667aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mmodel_run_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_run_locs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'location_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "# for day in date_list:\n",
    "#     VALIDATION_DATE = day # Date to use data upto and including\n",
    "\n",
    "#     RUN_TYPE = 'validation'\n",
    "#     MODEL_INPUTS_VERSION = 'production-runs/' + DATA_DATE\n",
    "#     SNAPSHOT_VERSION = 'production-runs/' + DATA_DATE\n",
    "#     DATESTAMP_LABEL = '2020_05_23_Europe' # Will want to change this.\n",
    "\n",
    "#     PEAK_FILE = '/ihme/covid-19/deaths/mobility_inputs/2020_04_20/peak_locs_april20_.csv'\n",
    "#     PEAK_DURATION_FILE = None\n",
    "#     R0_FILE = None\n",
    "#     LOCATION_SET_VERSION = 678\n",
    "#     r0_locs = []\n",
    "#     # Locations where no pseudo data is used\n",
    "#     NO_PSEUDO = [\n",
    "#         564, # South Dakota\n",
    "#         538, # Iowa\n",
    "#         # Mexican subnationals\n",
    "#         4644, 4657, 4651, 4663, 4665, 4667, 4669\n",
    "#     ]\n",
    "\n",
    "#     VALIDATION_FOLDER = re.sub(\"-\", \"_\", VALIDATION_DATE)\n",
    "#     CODE_DIR = os.path.abspath('../src/covid_model_deaths')\n",
    "#     OUTPUT_DIR = f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}/{VALIDATION_FOLDER}'\n",
    "#     if not os.path.exists(f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}'):\n",
    "#         os.mkdir(f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}')\n",
    "#     if not os.path.exists(OUTPUT_DIR):\n",
    "#         os.mkdir(OUTPUT_DIR)\n",
    "#     inputs = InputsContext(f'/ihme/covid-19/model-inputs/{MODEL_INPUTS_VERSION}')\n",
    "#     checkpoint = Checkpoint(OUTPUT_DIR)\n",
    "\n",
    "#     smooth_draw_path = f'{OUTPUT_DIR}/smoothed_euro_data.csv'\n",
    "#     raw_draw_path = f'{OUTPUT_DIR}/euro_data.csv'\n",
    "#     average_draw_path = f'{OUTPUT_DIR}/past_avg_smoothed_euro_data.csv'\n",
    "#     yesterday_draw_path = '/ihme/covid-19/deaths/prod/2020_05_22_Europe/smoothed_euro_data.csv'\n",
    "#     before_yesterday_draw_path = '/ihme/covid-19/deaths/prod/2020_05_19_Europe/smoothed_euro_data.csv'\n",
    "#     compare_average_path = '/ihme/covid-19/deaths/prod/2020_05_22_Europe/smoothed_euro_data.csv'\n",
    "\n",
    "#     print(f'Writing to {OUTPUT_DIR}')\n",
    "#     print(CODE_DIR)\n",
    "#     print(checkpoint)\n",
    "\n",
    "#     metadata = {}\n",
    "#     with open(f'/ihme/covid-19/model-inputs/{MODEL_INPUTS_VERSION}/metadata.yaml') as f:\n",
    "#         metadata['inputs_version'] = yaml.full_load(f)\n",
    "\n",
    "#     metadata['run_type'] = RUN_TYPE\n",
    "#     metadata['model_inputs_version'] = MODEL_INPUTS_VERSION\n",
    "#     metadata['snapshot_version'] = SNAPSHOT_VERSION\n",
    "#     metadata['datestamp_label'] = DATESTAMP_LABEL\n",
    "#     metadata['peak_file'] = PEAK_FILE\n",
    "#     metadata['location_set_version_id'] = LOCATION_SET_VERSION\n",
    "#     metadata['output_dir'] = OUTPUT_DIR\n",
    "#     metadata['no_pseudo'] = NO_PSEUDO\n",
    "#     metadata['average'] = {\n",
    "#         'yesterday': yesterday_draw_path,\n",
    "#         'before_yesterday': before_yesterday_draw_path\n",
    "#     }\n",
    "#     metadata['compare_average'] = compare_average_path\n",
    "\n",
    "#     with open(f'{OUTPUT_DIR}/metadata.yaml', 'w') as f:\n",
    "#         yaml.dump(metadata, f)\n",
    "\n",
    "#     def filter_data(data: pd.DataFrame, kind='full') -> pd.DataFrame:\n",
    "#         # manually adjust Iceland spike (0 deaths to 5 deaths to 0 deaths in March...)\n",
    "#         iceland = data['Country/Region'] == 'Iceland'\n",
    "#         iceland_spike = iceland & (data['Date'] == pd.Timestamp('2020-03-15'))\n",
    "#         if kind == 'full':\n",
    "#             data.loc[iceland_spike, ['Deaths', 'Death rate']] = 0\n",
    "#         elif kind == 'deaths':\n",
    "#             data = data.loc[~iceland_spike]\n",
    "#             min_iceland_date = data.loc[iceland, 'Date'].min()\n",
    "#             data.loc[iceland, 'Days'] = (data.loc[iceland, 'Date'] - min_iceland_date).dt.days\n",
    "\n",
    "#         catalonia  = data['location_id'] == 60368\n",
    "#         catalonia_spike = catalonia & (data['Date'] >= pd.Timestamp('2020-05-21'))\n",
    "#         data = data[~catalonia_spike]\n",
    "\n",
    "#         return data\n",
    "\n",
    "#     def get_locations(location_set_version_id):\n",
    "#         # get locaton_ids\n",
    "#         loc_df = get_location_metadata(location_set_id=111,\n",
    "#                                        location_set_version_id=location_set_version_id)\n",
    "\n",
    "#         # Drop any locations in the US and keep only most detailed for modeling\n",
    "#         most_detailed = loc_df['most_detailed'] == 1\n",
    "#         # non_us = ~loc_df['path_to_top_parent'].str.startswith('102,')\n",
    "#         keep_columns = ['location_id', 'location_ascii_name', 'parent_id', 'level', 'most_detailed']\n",
    "\n",
    "#         euro_df = loc_df.loc[most_detailed, keep_columns]\n",
    "#         euro_df = euro_df.rename(columns={'location_ascii_name':'Location'})\n",
    "\n",
    "#         # Add parents\n",
    "#         loc_df = loc_df[['location_id', 'location_ascii_name']]\n",
    "#         loc_df = loc_df.rename(columns={'location_id':'parent_id',\n",
    "#                                         'location_ascii_name':'Country/Region'})\n",
    "#         euro_df = euro_df.merge(loc_df)\n",
    "\n",
    "#         euro_df = euro_df.loc[:, ['location_id', 'Location', 'Country/Region', 'level']]\n",
    "#         return euro_df # don't like the name but probably easier to NOT change it.\n",
    "\n",
    "#     loc_df = get_locations(LOCATION_SET_VERSION)\n",
    "#     input_full_df = filter_data(inputs.load(MEASURES.full_data))\n",
    "#     input_death_df = filter_data(inputs.load(MEASURES.deaths), kind='deaths')\n",
    "\n",
    "#     # Subset to just dates for validation run\n",
    "#     input_full_df = input_full_df[input_full_df['Date'] <= VALIDATION_DATE]\n",
    "#     input_death_df = input_death_df[input_death_df['Date'] <= VALIDATION_DATE]\n",
    "\n",
    "#     input_age_pop_df = inputs.load(MEASURES.age_pop)\n",
    "#     input_age_death_df = inputs.load(MEASURES.age_death)\n",
    "#     smoothed_case_df, smoothed_death_df = runner.get_smoothed(input_full_df)\n",
    "\n",
    "#     # save cases for viz\n",
    "#     smoothed_case_df[[COLUMNS.location_id, COLUMNS.date, 'ln(case rate)', 'population']].to_csv(\n",
    "#         f'{OUTPUT_DIR}/smoothed_cases.csv', index=False\n",
    "#     )\n",
    "\n",
    "#     # Save pops for Bobby.\n",
    "#     pop_df = input_age_pop_df.merge(loc_df).reset_index(drop=True)\n",
    "#     pop_df[['location_id', 'Location', 'age_group', 'population']].to_csv(f'{OUTPUT_DIR}/pops.csv', index=False)\n",
    "\n",
    "#     checkpoint.write('location', loc_df)\n",
    "#     checkpoint.write('full_data', input_full_df)\n",
    "#     checkpoint.write('deaths', input_death_df)\n",
    "#     checkpoint.write('smoothed_cases', smoothed_case_df)\n",
    "#     checkpoint.write('smoothed_deaths', smoothed_death_df)\n",
    "#     checkpoint.write('age_pop', input_age_pop_df)\n",
    "#     checkpoint.write('age_death', input_age_death_df)\n",
    "\n",
    "#     #%%time\n",
    "#     full_df = checkpoint.load('full_data')\n",
    "#     death_df = checkpoint.load('deaths')\n",
    "#     age_pop_df = checkpoint.load('age_pop')\n",
    "#     age_death_df = checkpoint.load('age_death')\n",
    "\n",
    "#     backcast_location_ids = runner.get_backcast_location_ids(full_df, most_detailed=False)\n",
    "#     cases_and_backcast_deaths_df = runner.make_cases_and_backcast_deaths(full_df, death_df, age_pop_df, \n",
    "#                                                                          age_death_df, backcast_location_ids, \n",
    "#                                                                          subnat=False)\n",
    "\n",
    "#     cases_and_backcast_deaths_df.to_csv(f'{OUTPUT_DIR}/backcast_for_case_to_death.csv', index=False)\n",
    "#     checkpoint.write('cases_and_backcast_deaths', cases_and_backcast_deaths_df)\n",
    "\n",
    "#     #%%time\n",
    "#     cases_and_backcast_deaths_df = checkpoint.load('cases_and_backcast_deaths')\n",
    "#     loc_df = checkpoint.load('location')\n",
    "\n",
    "#     # loc_df needs to be updated with locations that have death data\n",
    "#     # must have > 0/NaN deaths \n",
    "# #     model_run_locs = input_death_df[input_death_df['Deaths'].sum() >0]\n",
    "# #     model_run_locs = input_death_df['location_id'].unique()\n",
    "    \n",
    "#     model_run_locs = cases_and_backcast_deaths_df[cases_and_backcast_deaths_df['Deaths'].notnull() & cases_and_backcast_deaths_df['Confirmed case rate'].notnull()]\n",
    "#     model_run_locs = model_run_locs['location_id'].unique()\n",
    "    \n",
    "#     stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal',\n",
       "       'Western Cape', 'Alabama', 'Alaska', 'Arizona', 'Arkansas',\n",
       "       'California', 'Colorado', 'Connecticut', 'Delaware',\n",
       "       'District of Columbia', 'Florida', 'Georgia', 'Hawaii', 'Idaho',\n",
       "       'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana',\n",
       "       'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota',\n",
       "       'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada',\n",
       "       'New Hampshire', 'New Jersey', 'New Mexico', 'New York',\n",
       "       'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon',\n",
       "       'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota',\n",
       "       'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia',\n",
       "       'West Virginia', 'Wisconsin', 'Baja California',\n",
       "       'Baja California Sur', 'Campeche', 'Coahuila', 'Chiapas',\n",
       "       'Chihuahua', 'Mexico City', 'Durango', 'Guanajuato', 'Guerrero',\n",
       "       'Hidalgo', 'Jalisco', 'Mexico', 'Michoacan de Ocampo', 'Morelos',\n",
       "       'Nayarit', 'Nuevo Leon', 'Oaxaca', 'Puebla', 'Queretaro',\n",
       "       'Quintana Roo', 'San Luis Potosi', 'Sinaloa', 'Sonora', 'Tabasco',\n",
       "       'Tamaulipas', 'Tlaxcala', 'Veracruz de Ignacio de la Llave',\n",
       "       'Yucatan', 'Acre', 'Alagoas', 'Amazonas', 'Amapa', 'Bahia',\n",
       "       'Ceara', 'Distrito Federal', 'Espirito Santo', 'Goias', 'Maranhao',\n",
       "       'Minas Gerais', 'Mato Grosso do Sul', 'Mato Grosso', 'Para',\n",
       "       'Paraiba', 'Parana', 'Pernambuco', 'Piaui', 'Rio de Janeiro',\n",
       "       'Rio Grande do Norte', 'Rondonia', 'Roraima', 'Rio Grande do Sul',\n",
       "       'Santa Catarina', 'Sergipe', 'Sao Paulo', 'Delhi', 'Gujarat',\n",
       "       'Jammu & Kashmir and Ladakh', 'Madhya Pradesh', 'Maharashtra',\n",
       "       'Punjab', 'Telangana', 'Piemonte', \"Valle d'Aosta\", 'Liguria',\n",
       "       'Lombardia', 'Provincia autonoma di Bolzano',\n",
       "       'Provincia autonoma di Trento', 'Veneto', 'Friuli-Venezia Giulia',\n",
       "       'Emilia-Romagna', 'Toscana', 'Umbria', 'Marche', 'Lazio',\n",
       "       'Abruzzo', 'Molise', 'Campania', 'Puglia', 'Basilicata',\n",
       "       'Calabria', 'Sicilia', 'Sardegna', 'Balochistan',\n",
       "       'Gilgit-Baltistan', 'Khyber Pakhtunkhwa', 'Sindh', 'Wuhan',\n",
       "       'Baden-Wurttemberg', 'Bavaria', 'Berlin', 'Brandenburg', 'Bremen',\n",
       "       'Hamburg', 'Hesse', 'Lower Saxony', 'Mecklenburg-Vorpommern',\n",
       "       'North Rhine-Westphalia', 'Rhineland-Palatinate', 'Saarland',\n",
       "       'Saxony-Anhalt', 'Saxony', 'Schleswig-Holstein', 'Thuringia',\n",
       "       'Andalucia', 'Aragon', 'Cantabria', 'Castilla-La Mancha',\n",
       "       'Community of Madrid', 'Extremadura', 'Balearic Islands',\n",
       "       'Canary Islands', 'Asturias', 'Murcia', 'Castile and Leon',\n",
       "       'Catalonia', 'Ceuta', 'Navarre', 'Valencian Community',\n",
       "       'Basque Country', 'La Rioja', 'Galicia', 'Alberta',\n",
       "       'British Columbia', 'Manitoba', 'Newfoundland and Labrador',\n",
       "       'Nova Scotia', 'Ontario', 'Quebec', 'Saskatchewan',\n",
       "       'Spokane County', 'King and Snohomish Counties',\n",
       "       'Washington except for King, Snohomish, and Spokane Counties'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#     model_run_locs = cases_and_backcast_deaths_df[cases_and_backcast_deaths_df['Deaths'].notnull() & cases_and_backcast_deaths_df['Confirmed case rate'].notnull()]\n",
    "#     model_run_locs = model_run_locs['Province/State'].unique()\n",
    "#     model_run_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-03 15:40:31.084 | DEBUG    | covid_model_deaths.deaths_io.checkpoint:_setup_checkpoint_dir:45 - Making checkpoint directory at /ihme/covid-19/deaths/validation/2020_05_29/2020_04_24/checkpoint\n",
      "2020-06-03 15:40:31.136 | DEBUG    | covid_model_deaths.deaths_io.inputs:load:40 - Loading full_data.csv from /ihme/covid-19/model-inputs/2020_05_29.01.\n",
      "2020-06-03 15:40:31.194 | DEBUG    | covid_model_deaths.deaths_io.inputs:load:40 - Loading deaths.csv from /ihme/covid-19/model-inputs/2020_05_29.01.\n",
      "2020-06-03 15:40:31.235 | DEBUG    | covid_model_deaths.deaths_io.inputs:load:40 - Loading age_pop.csv from /ihme/covid-19/model-inputs/2020_05_29.01.\n",
      "2020-06-03 15:40:31.243 | DEBUG    | covid_model_deaths.deaths_io.inputs:load:40 - Loading age_death.csv from /ihme/covid-19/model-inputs/2020_05_29.01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /ihme/covid-19/deaths/validation/2020_05_29/2020_04_24\n",
      "/ihme/code/covid-19/user/ctroeger/covid-model-deaths/src/covid_model_deaths\n",
      "Checkpoint(/ihme/covid-19/deaths/validation/2020_05_29/2020_04_24/checkpoint)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-03 15:40:45.584 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading full_data from in memory cache.\n",
      "2020-06-03 15:40:45.587 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading deaths from in memory cache.\n",
      "2020-06-03 15:40:45.588 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading age_pop from in memory cache.\n",
      "2020-06-03 15:40:45.589 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading age_death from in memory cache.\n",
      "  0%|          | 0/324 [00:12<?, ?it/s]Process ForkPoolWorker-258:\n",
      "Process ForkPoolWorker-259:\n",
      "Process ForkPoolWorker-257:\n",
      "Process ForkPoolWorker-260:\n",
      "Process ForkPoolWorker-256:\n",
      "Process ForkPoolWorker-254:\n",
      "Process ForkPoolWorker-250:\n",
      "Process ForkPoolWorker-253:\n",
      "Process ForkPoolWorker-255:\n",
      "Process ForkPoolWorker-248:\n",
      "Process ForkPoolWorker-247:\n",
      "Process ForkPoolWorker-243:\n",
      "Process ForkPoolWorker-252:\n",
      "Process ForkPoolWorker-246:\n",
      "Process ForkPoolWorker-251:\n",
      "Process ForkPoolWorker-242:\n",
      "Process ForkPoolWorker-241:\n",
      "Process ForkPoolWorker-249:\n",
      "Process ForkPoolWorker-245:\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/code/ctroeger/miniconda/envs/covid-deaths-2020-06-03_10-03-29/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/code/ctroeger/miniconda/envs/covid-deaths-2020-06-03_10-03-29/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/code/ctroeger/miniconda/envs/covid-deaths-2020-06-03_10-03-29/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/code/ctroeger/miniconda/envs/covid-deaths-2020-06-03_10-03-29/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/code/ctroeger/miniconda/envs/covid-deaths-2020-06-03_10-03-29/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/code/ctroeger/miniconda/envs/covid-deaths-2020-06-03_10-03-29/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/ihme/code/covid-19/user/ctroeger/covid-model-deaths/src/covid_model_deaths/runner.py\", line 65, in backcast_deaths\n",
      "    rate_threshold=cmd_globals.LN_MORTALITY_RATE_THRESHOLD)\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/code/ctroeger/miniconda/envs/covid-deaths-2020-06-03_10-03-29/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/code/ctroeger/miniconda/envs/covid-deaths-2020-06-03_10-03-29/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/code/ctroeger/miniconda/envs/covid-deaths-2020-06-03_10-03-29/lib/python3.6/site-packages/pandas/core/generic.py\", line 5286, in __setattr__\n",
      "    object.__getattribute__(self, name)\n",
      "  File \"/share/code/ctroeger/miniconda/envs/covid-deaths-2020-06-03_10-03-29/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/share/code/ctroeger/miniconda/envs/covid-deaths-2020-06-03_10-03-29/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/share/code/ctroeger/miniconda/envs/covid-deaths-2020-06-03_10-03-29/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1e0d703cb56a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    139\u001b[0m     cases_and_backcast_deaths_df = runner.make_cases_and_backcast_deaths(full_df, death_df, age_pop_df, \n\u001b[1;32m    140\u001b[0m                                                                          \u001b[0mage_death_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackcast_location_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                                                                          subnat=False)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mcases_and_backcast_deaths_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{OUTPUT_DIR}/backcast_for_case_to_death.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ihme/code/covid-19/user/ctroeger/covid-model-deaths/src/covid_model_deaths/runner.py\u001b[0m in \u001b[0;36mmake_cases_and_backcast_deaths\u001b[0;34m(full_df, death_df, age_pop_df, age_death_df, location_ids, subnat)\u001b[0m\n\u001b[1;32m     29\u001b[0m                                    \u001b[0mage_pop_df\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_death_df\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                    location_ids: List[int], subnat: bool = True) -> pd.DataFrame:\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mbackcast_deaths_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackcast_deaths_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeath_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_pop_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_death_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubnat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     full_df_columns = [COLUMNS.location_id, COLUMNS.state, COLUMNS.country, COLUMNS.date,\n",
      "\u001b[0;32m/ihme/code/covid-19/user/ctroeger/covid-model-deaths/src/covid_model_deaths/runner.py\u001b[0m in \u001b[0;36mbackcast_deaths_parallel\u001b[0;34m(location_ids, death_df, age_pop_df, age_death, subnat)\u001b[0m\n\u001b[1;32m     50\u001b[0m                                   subnat=subnat)\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mbackcast_deaths_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_combiner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackcast_deaths_dfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/code/ctroeger/miniconda/envs/covid-deaths-2020-06-03_10-03-29/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/code/ctroeger/miniconda/envs/covid-deaths-2020-06-03_10-03-29/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/code/ctroeger/miniconda/envs/covid-deaths-2020-06-03_10-03-29/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for day in date_list:\n",
    "#     VALIDATION_DATE = day # Date to use data upto and including\n",
    "\n",
    "#     RUN_TYPE = 'validation'\n",
    "#     MODEL_INPUTS_VERSION = 'production-runs/' + DATA_DATE\n",
    "#     SNAPSHOT_VERSION = 'production-runs/' + DATA_DATE\n",
    "#     DATESTAMP_LABEL = '2020_05_23_Europe' # Will want to change this.\n",
    "\n",
    "#     PEAK_FILE = '/ihme/covid-19/deaths/mobility_inputs/2020_04_20/peak_locs_april20_.csv'\n",
    "#     PEAK_DURATION_FILE = None\n",
    "#     R0_FILE = None\n",
    "#     LOCATION_SET_VERSION = 678\n",
    "#     r0_locs = []\n",
    "#     # Locations where no pseudo data is used\n",
    "#     NO_PSEUDO = [\n",
    "#         564, # South Dakota\n",
    "#         538, # Iowa\n",
    "#         # Mexican subnationals\n",
    "#         4644, 4657, 4651, 4663, 4665, 4667, 4669\n",
    "#     ]\n",
    "\n",
    "#     VALIDATION_FOLDER = re.sub(\"-\", \"_\", VALIDATION_DATE)\n",
    "#     CODE_DIR = os.path.abspath('../src/covid_model_deaths')\n",
    "#     OUTPUT_DIR = f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}/{VALIDATION_FOLDER}'\n",
    "#     if not os.path.exists(f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}'):\n",
    "#         os.mkdir(f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}')\n",
    "#     if not os.path.exists(OUTPUT_DIR):\n",
    "#         os.mkdir(OUTPUT_DIR)\n",
    "#     inputs = InputsContext(f'/ihme/covid-19/model-inputs/{MODEL_INPUTS_VERSION}')\n",
    "#     checkpoint = Checkpoint(OUTPUT_DIR)\n",
    "\n",
    "#     smooth_draw_path = f'{OUTPUT_DIR}/smoothed_euro_data.csv'\n",
    "#     raw_draw_path = f'{OUTPUT_DIR}/euro_data.csv'\n",
    "#     average_draw_path = f'{OUTPUT_DIR}/past_avg_smoothed_euro_data.csv'\n",
    "#     yesterday_draw_path = '/ihme/covid-19/deaths/prod/2020_05_22_Europe/smoothed_euro_data.csv'\n",
    "#     before_yesterday_draw_path = '/ihme/covid-19/deaths/prod/2020_05_19_Europe/smoothed_euro_data.csv'\n",
    "#     compare_average_path = '/ihme/covid-19/deaths/prod/2020_05_22_Europe/smoothed_euro_data.csv'\n",
    "\n",
    "#     print(f'Writing to {OUTPUT_DIR}')\n",
    "#     print(CODE_DIR)\n",
    "#     print(checkpoint)\n",
    "\n",
    "#     metadata = {}\n",
    "#     with open(f'/ihme/covid-19/model-inputs/{MODEL_INPUTS_VERSION}/metadata.yaml') as f:\n",
    "#         metadata['inputs_version'] = yaml.full_load(f)\n",
    "\n",
    "#     metadata['run_type'] = RUN_TYPE\n",
    "#     metadata['model_inputs_version'] = MODEL_INPUTS_VERSION\n",
    "#     metadata['snapshot_version'] = SNAPSHOT_VERSION\n",
    "#     metadata['datestamp_label'] = DATESTAMP_LABEL\n",
    "#     metadata['peak_file'] = PEAK_FILE\n",
    "#     metadata['location_set_version_id'] = LOCATION_SET_VERSION\n",
    "#     metadata['output_dir'] = OUTPUT_DIR\n",
    "#     metadata['no_pseudo'] = NO_PSEUDO\n",
    "#     metadata['average'] = {\n",
    "#         'yesterday': yesterday_draw_path,\n",
    "#         'before_yesterday': before_yesterday_draw_path\n",
    "#     }\n",
    "#     metadata['compare_average'] = compare_average_path\n",
    "\n",
    "#     with open(f'{OUTPUT_DIR}/metadata.yaml', 'w') as f:\n",
    "#         yaml.dump(metadata, f)\n",
    "\n",
    "#     def filter_data(data: pd.DataFrame, kind='full') -> pd.DataFrame:\n",
    "#         # manually adjust Iceland spike (0 deaths to 5 deaths to 0 deaths in March...)\n",
    "#         iceland = data['Country/Region'] == 'Iceland'\n",
    "#         iceland_spike = iceland & (data['Date'] == pd.Timestamp('2020-03-15'))\n",
    "#         if kind == 'full':\n",
    "#             data.loc[iceland_spike, ['Deaths', 'Death rate']] = 0\n",
    "#         elif kind == 'deaths':\n",
    "#             data = data.loc[~iceland_spike]\n",
    "#             min_iceland_date = data.loc[iceland, 'Date'].min()\n",
    "#             data.loc[iceland, 'Days'] = (data.loc[iceland, 'Date'] - min_iceland_date).dt.days\n",
    "\n",
    "#         catalonia  = data['location_id'] == 60368\n",
    "#         catalonia_spike = catalonia & (data['Date'] >= pd.Timestamp('2020-05-21'))\n",
    "#         data = data[~catalonia_spike]\n",
    "\n",
    "#         return data\n",
    "\n",
    "#     def get_locations(location_set_version_id):\n",
    "#         # get locaton_ids\n",
    "#         loc_df = get_location_metadata(location_set_id=111,\n",
    "#                                        location_set_version_id=location_set_version_id)\n",
    "\n",
    "#         # Drop any locations in the US and keep only most detailed for modeling\n",
    "#         most_detailed = loc_df['most_detailed'] == 1\n",
    "#         # non_us = ~loc_df['path_to_top_parent'].str.startswith('102,')\n",
    "#         keep_columns = ['location_id', 'location_ascii_name', 'parent_id', 'level', 'most_detailed']\n",
    "\n",
    "#         euro_df = loc_df.loc[most_detailed, keep_columns]\n",
    "#         euro_df = euro_df.rename(columns={'location_ascii_name':'Location'})\n",
    "\n",
    "#         # Add parents\n",
    "#         loc_df = loc_df[['location_id', 'location_ascii_name']]\n",
    "#         loc_df = loc_df.rename(columns={'location_id':'parent_id',\n",
    "#                                         'location_ascii_name':'Country/Region'})\n",
    "#         euro_df = euro_df.merge(loc_df)\n",
    "\n",
    "#         euro_df = euro_df.loc[:, ['location_id', 'Location', 'Country/Region', 'level']]\n",
    "#         return euro_df # don't like the name but probably easier to NOT change it.\n",
    "\n",
    "#     loc_df = get_locations(LOCATION_SET_VERSION)\n",
    "#     input_full_df = filter_data(inputs.load(MEASURES.full_data))\n",
    "#     input_death_df = filter_data(inputs.load(MEASURES.deaths), kind='deaths')\n",
    "\n",
    "#     # Subset to just dates for validation run\n",
    "#     input_full_df = input_full_df[input_full_df['Date'] <= VALIDATION_DATE]\n",
    "#     input_death_df = input_death_df[input_death_df['Date'] <= VALIDATION_DATE]\n",
    "\n",
    "#     input_age_pop_df = inputs.load(MEASURES.age_pop)\n",
    "#     input_age_death_df = inputs.load(MEASURES.age_death)\n",
    "#     smoothed_case_df, smoothed_death_df = runner.get_smoothed(input_full_df)\n",
    "\n",
    "#     # save cases for viz\n",
    "#     smoothed_case_df[[COLUMNS.location_id, COLUMNS.date, 'ln(case rate)', 'population']].to_csv(\n",
    "#         f'{OUTPUT_DIR}/smoothed_cases.csv', index=False\n",
    "#     )\n",
    "\n",
    "#     # Save pops for Bobby.\n",
    "#     pop_df = input_age_pop_df.merge(loc_df).reset_index(drop=True)\n",
    "#     pop_df[['location_id', 'Location', 'age_group', 'population']].to_csv(f'{OUTPUT_DIR}/pops.csv', index=False)\n",
    "\n",
    "#     checkpoint.write('location', loc_df)\n",
    "#     checkpoint.write('full_data', input_full_df)\n",
    "#     checkpoint.write('deaths', input_death_df)\n",
    "#     checkpoint.write('smoothed_cases', smoothed_case_df)\n",
    "#     checkpoint.write('smoothed_deaths', smoothed_death_df)\n",
    "#     checkpoint.write('age_pop', input_age_pop_df)\n",
    "#     checkpoint.write('age_death', input_age_death_df)\n",
    "\n",
    "#     #%%time\n",
    "#     full_df = checkpoint.load('full_data')\n",
    "#     death_df = checkpoint.load('deaths')\n",
    "#     age_pop_df = checkpoint.load('age_pop')\n",
    "#     age_death_df = checkpoint.load('age_death')\n",
    "\n",
    "#     backcast_location_ids = runner.get_backcast_location_ids(full_df, most_detailed=False)\n",
    "#     cases_and_backcast_deaths_df = runner.make_cases_and_backcast_deaths(full_df, death_df, age_pop_df, \n",
    "#                                                                          age_death_df, backcast_location_ids, \n",
    "#                                                                          subnat=False)\n",
    "\n",
    "#     cases_and_backcast_deaths_df.to_csv(f'{OUTPUT_DIR}/backcast_for_case_to_death.csv', index=False)\n",
    "#     checkpoint.write('cases_and_backcast_deaths', cases_and_backcast_deaths_df)\n",
    "\n",
    "#     #%%time\n",
    "#     cases_and_backcast_deaths_df = checkpoint.load('cases_and_backcast_deaths')\n",
    "#     loc_df = checkpoint.load('location')\n",
    "\n",
    "#     # loc_df needs to be updated with locations that have death data\n",
    "#     # must have > 0/NaN deaths \n",
    "# #     model_run_locs = input_death_df[input_death_df['Deaths'].sum() >0]\n",
    "# #     model_run_locs = input_death_df['location_id'].unique()\n",
    "    \n",
    "#     model_run_locs = cases_and_backcast_deaths_df[cases_and_backcast_deaths_df['Deaths'].notnull() & cases_and_backcast_deaths_df['Confirmed case rate'].isna()]\n",
    "#     model_run_locs = model_run_locs['location_id'].unique()\n",
    "# #    model_run_locs\n",
    "    \n",
    "#     cases_and_backcast_deaths_df = cases_and_backcast_deaths_df[cases_and_backcast_deaths_df['location_id'].isin(model_run_locs)]\n",
    "#     loc_df = loc_df[loc_df['location_id'].isin(model_run_locs)]\n",
    "\n",
    "#     checkpoint.write('location', loc_df)\n",
    "\n",
    "#     threshold_dates = runner.impute_death_threshold(cases_and_backcast_deaths_df,\n",
    "#                                                     loc_df)\n",
    "#     threshold_dates.to_csv(f'{OUTPUT_DIR}/threshold_dates.csv', index=False)\n",
    "#     checkpoint.write('threshold_dates', threshold_dates)\n",
    "\n",
    "#     smoothed_death_df = checkpoint.load('smoothed_deaths')\n",
    "#     threshold_dates = checkpoint.load('threshold_dates')\n",
    "\n",
    "#     date_mean_df = runner.make_date_mean_df(threshold_dates)\n",
    "#     last_day_df = runner.make_last_day_df(smoothed_death_df,date_mean_df)\n",
    "#     last_day_df.to_csv(f'{OUTPUT_DIR}/last_day.csv', index=False)\n",
    "\n",
    "#     checkpoint.write('date_mean', date_mean_df)\n",
    "#     checkpoint.write('last_day', last_day_df)\n",
    "\n",
    "#     full_df = checkpoint.load('full_data')\n",
    "#     loc_df = checkpoint.load('location')\n",
    "\n",
    "#     df_to_run = full_df.loc[full_df[COLUMNS.location_id].isin(loc_df[COLUMNS.location_id].to_list())]\n",
    "#     dcr_df, dhr_df, leading_indicator_df = runner.make_leading_indicator(\n",
    "#         df_to_run,\n",
    "#         SNAPSHOT_VERSION\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Japan', 'Ecuador', 'Eastern Cape', 'Free State', 'Gauteng',\n",
       "       'KwaZulu-Natal', 'Western Cape', 'Sindh', 'Andalucia', 'Aragon',\n",
       "       'Cantabria', 'Castilla-La Mancha', 'Extremadura', 'Asturias',\n",
       "       'Murcia', 'Ceuta', 'Navarre', 'Valencian Community',\n",
       "       'Basque Country', 'La Rioja', 'Galicia', 'Indonesia', 'Malaysia',\n",
       "       'Philippines', 'Sri Lanka', 'Thailand', 'Armenia', 'Azerbaijan',\n",
       "       'Georgia', 'Kazakhstan', 'Kyrgyzstan', 'Albania',\n",
       "       'Bosnia and Herzegovina', 'Bulgaria', 'Croatia', 'Czechia',\n",
       "       'Hungary', 'North Macedonia', 'Montenegro', 'Poland', 'Romania',\n",
       "       'Serbia', 'Slovakia', 'Slovenia', 'Belarus', 'Estonia', 'Latvia',\n",
       "       'Lithuania', 'Republic of Moldova', 'Russian Federation',\n",
       "       'Ukraine', 'Republic of Korea', 'Singapore', 'Australia',\n",
       "       'New Zealand', 'Andorra', 'Austria', 'Belgium', 'Cyprus',\n",
       "       'Denmark', 'Finland', 'France', 'Greece', 'Iceland', 'Ireland',\n",
       "       'Israel', 'Luxembourg', 'Malta', 'Netherlands', 'Norway',\n",
       "       'Portugal', 'Sweden', 'Switzerland', 'United Kingdom', 'Argentina',\n",
       "       'Chile', 'Uruguay', 'Antigua and Barbuda', 'Bahamas', 'Barbados',\n",
       "       'Cuba', 'Dominican Republic', 'Guyana', 'Jamaica',\n",
       "       'Trinidad and Tobago', 'Bolivia (Plurinational State of)', 'Peru',\n",
       "       'Colombia', 'Costa Rica', 'El Salvador', 'Guatemala', 'Honduras',\n",
       "       'Panama', 'Venezuela (Bolivarian Republic of)', 'Paraguay',\n",
       "       'Algeria', 'Bahrain', 'Egypt', 'Iran (Islamic Republic of)',\n",
       "       'Iraq', 'Jordan', 'Kuwait', 'Lebanon', 'Morocco', 'Oman', 'Qatar',\n",
       "       'Saudi Arabia', 'Tunisia', 'Turkey', 'United Arab Emirates',\n",
       "       'Afghanistan', 'Bangladesh', 'Congo', 'Mauritius', 'Burkina Faso',\n",
       "       'Cameroon', 'Liberia', 'Mali', 'Niger', 'Togo', 'Guam', 'Monaco',\n",
       "       'Puerto Rico', 'San Marino', 'United States Virgin Islands',\n",
       "       'Lombardia', 'Community of Madrid', 'Wuhan'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem = df_to_run[df_to_run['Confirmed case rate'].isna()]\n",
    "# problem = cases_and_backcast_deaths_df[cases_and_backcast_deaths_df['Confirmed case rate'].isna()]\n",
    "# problem['Province/State'].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-05 21:29:41.984 | DEBUG    | covid_model_deaths.deaths_io.checkpoint:_setup_checkpoint_dir:45 - Making checkpoint directory at /ihme/covid-19/deaths/validation/2020_05_29/2020_04_07/checkpoint\n",
      "2020-06-05 21:29:42.039 | DEBUG    | covid_model_deaths.deaths_io.inputs:load:40 - Loading full_data.csv from /ihme/covid-19/model-inputs/2020_05_29.01.\n",
      "2020-06-05 21:29:42.088 | DEBUG    | covid_model_deaths.deaths_io.inputs:load:40 - Loading deaths.csv from /ihme/covid-19/model-inputs/2020_05_29.01.\n",
      "2020-06-05 21:29:42.125 | DEBUG    | covid_model_deaths.deaths_io.inputs:load:40 - Loading age_pop.csv from /ihme/covid-19/model-inputs/2020_05_29.01.\n",
      "2020-06-05 21:29:42.131 | DEBUG    | covid_model_deaths.deaths_io.inputs:load:40 - Loading age_death.csv from /ihme/covid-19/model-inputs/2020_05_29.01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /ihme/covid-19/deaths/validation/2020_05_29/2020_04_07\n",
      "/ihme/code/covid-19/user/ctroeger/covid-model-deaths/src/covid_model_deaths\n",
      "Checkpoint(/ihme/covid-19/deaths/validation/2020_05_29/2020_04_07/checkpoint)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-05 21:29:54.366 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading full_data from in memory cache.\n",
      "2020-06-05 21:29:54.368 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading deaths from in memory cache.\n",
      "2020-06-05 21:29:54.369 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading age_pop from in memory cache.\n",
      "2020-06-05 21:29:54.370 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading age_death from in memory cache.\n",
      "100%|██████████| 279/279 [01:57<00:00,  2.37it/s]\n",
      "2020-06-05 21:31:53.085 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading cases_and_backcast_deaths from in memory cache.\n",
      "2020-06-05 21:31:53.086 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading location from in memory cache.\n",
      "2020-06-05 21:31:53.128 | WARNING  | covid_model_deaths.deaths_io.checkpoint:write:24 - Overwriting location in checkpoint data.\n",
      "100%|██████████| 107/107 [00:05<00:00, 18.66it/s]\n",
      "2020-06-05 21:31:59.968 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading smoothed_deaths from in memory cache.\n",
      "2020-06-05 21:31:59.970 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading threshold_dates from in memory cache.\n",
      "2020-06-05 21:32:02.528 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading full_data from in memory cache.\n",
      "2020-06-05 21:32:02.531 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading location from in memory cache.\n",
      "2020-06-05 21:32:09.714 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading full_data from in memory cache.\n",
      "2020-06-05 21:32:09.716 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading deaths from in memory cache.\n",
      "2020-06-05 21:32:09.717 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading age_pop from in memory cache.\n",
      "2020-06-05 21:32:09.718 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading age_death from in memory cache.\n",
      "2020-06-05 21:32:09.719 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading date_mean from in memory cache.\n",
      "2020-06-05 21:32:09.719 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading last_day from in memory cache.\n",
      "2020-06-05 21:32:09.720 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading leading_indicator from in memory cache.\n",
      "2020-06-05 21:32:09.721 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading location from in memory cache.\n",
      " 73%|███████▎  | 78/107 [11:13<04:03,  8.39s/it]"
     ]
    }
   ],
   "source": [
    "for day in date_list:\n",
    "    VALIDATION_DATE = day # Date to use data upto and including\n",
    "\n",
    "    RUN_TYPE = 'validation'\n",
    "    MODEL_INPUTS_VERSION = 'production-runs/' + DATA_DATE\n",
    "    SNAPSHOT_VERSION = 'production-runs/' + DATA_DATE\n",
    "    DATESTAMP_LABEL = '2020_05_23_Europe' # Will want to change this.\n",
    "\n",
    "    PEAK_FILE = '/ihme/covid-19/deaths/mobility_inputs/2020_04_20/peak_locs_april20_.csv'\n",
    "    PEAK_DURATION_FILE = None\n",
    "    R0_FILE = None\n",
    "    LOCATION_SET_VERSION = 678\n",
    "    r0_locs = []\n",
    "    # Locations where no pseudo data is used\n",
    "    NO_PSEUDO = [\n",
    "        564, # South Dakota\n",
    "        538, # Iowa\n",
    "        # Mexican subnationals\n",
    "        4644, 4657, 4651, 4663, 4665, 4667, 4669\n",
    "    ]\n",
    "\n",
    "    VALIDATION_FOLDER = re.sub(\"-\", \"_\", VALIDATION_DATE)\n",
    "    CODE_DIR = os.path.abspath('../src/covid_model_deaths')\n",
    "    OUTPUT_DIR = f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}/{VALIDATION_FOLDER}'\n",
    "    if not os.path.exists(f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}'):\n",
    "        os.mkdir(f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}')\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.mkdir(OUTPUT_DIR)\n",
    "    inputs = InputsContext(f'/ihme/covid-19/model-inputs/{MODEL_INPUTS_VERSION}')\n",
    "    checkpoint = Checkpoint(OUTPUT_DIR)\n",
    "\n",
    "    smooth_draw_path = f'{OUTPUT_DIR}/smoothed_euro_data.csv'\n",
    "    raw_draw_path = f'{OUTPUT_DIR}/euro_data.csv'\n",
    "    average_draw_path = f'{OUTPUT_DIR}/past_avg_smoothed_euro_data.csv'\n",
    "    yesterday_draw_path = '/ihme/covid-19/deaths/prod/2020_05_22_Europe/smoothed_euro_data.csv'\n",
    "    before_yesterday_draw_path = '/ihme/covid-19/deaths/prod/2020_05_19_Europe/smoothed_euro_data.csv'\n",
    "    compare_average_path = '/ihme/covid-19/deaths/prod/2020_05_22_Europe/smoothed_euro_data.csv'\n",
    "\n",
    "    print(f'Writing to {OUTPUT_DIR}')\n",
    "    print(CODE_DIR)\n",
    "    print(checkpoint)\n",
    "\n",
    "    metadata = {}\n",
    "    with open(f'/ihme/covid-19/model-inputs/{MODEL_INPUTS_VERSION}/metadata.yaml') as f:\n",
    "        metadata['inputs_version'] = yaml.full_load(f)\n",
    "\n",
    "    metadata['run_type'] = RUN_TYPE\n",
    "    metadata['model_inputs_version'] = MODEL_INPUTS_VERSION\n",
    "    metadata['snapshot_version'] = SNAPSHOT_VERSION\n",
    "    metadata['datestamp_label'] = DATESTAMP_LABEL\n",
    "    metadata['peak_file'] = PEAK_FILE\n",
    "    metadata['location_set_version_id'] = LOCATION_SET_VERSION\n",
    "    metadata['output_dir'] = OUTPUT_DIR\n",
    "    metadata['no_pseudo'] = NO_PSEUDO\n",
    "    metadata['average'] = {\n",
    "        'yesterday': yesterday_draw_path,\n",
    "        'before_yesterday': before_yesterday_draw_path\n",
    "    }\n",
    "    metadata['compare_average'] = compare_average_path\n",
    "\n",
    "    with open(f'{OUTPUT_DIR}/metadata.yaml', 'w') as f:\n",
    "        yaml.dump(metadata, f)\n",
    "\n",
    "    def filter_data(data: pd.DataFrame, kind='full') -> pd.DataFrame:\n",
    "        # manually adjust Iceland spike (0 deaths to 5 deaths to 0 deaths in March...)\n",
    "        iceland = data['Country/Region'] == 'Iceland'\n",
    "        iceland_spike = iceland & (data['Date'] == pd.Timestamp('2020-03-15'))\n",
    "        if kind == 'full':\n",
    "            data.loc[iceland_spike, ['Deaths', 'Death rate']] = 0\n",
    "        elif kind == 'deaths':\n",
    "            data = data.loc[~iceland_spike]\n",
    "            min_iceland_date = data.loc[iceland, 'Date'].min()\n",
    "            data.loc[iceland, 'Days'] = (data.loc[iceland, 'Date'] - min_iceland_date).dt.days\n",
    "\n",
    "        catalonia  = data['location_id'] == 60368\n",
    "        catalonia_spike = catalonia & (data['Date'] >= pd.Timestamp('2020-05-21'))\n",
    "        data = data[~catalonia_spike]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def get_locations(location_set_version_id):\n",
    "        # get locaton_ids\n",
    "        loc_df = get_location_metadata(location_set_id=111,\n",
    "                                       location_set_version_id=location_set_version_id)\n",
    "\n",
    "        # Drop any locations in the US and keep only most detailed for modeling\n",
    "        most_detailed = loc_df['most_detailed'] == 1\n",
    "        # non_us = ~loc_df['path_to_top_parent'].str.startswith('102,')\n",
    "        keep_columns = ['location_id', 'location_ascii_name', 'parent_id', 'level', 'most_detailed']\n",
    "\n",
    "        euro_df = loc_df.loc[most_detailed, keep_columns]\n",
    "        euro_df = euro_df.rename(columns={'location_ascii_name':'Location'})\n",
    "\n",
    "        # Add parents\n",
    "        loc_df = loc_df[['location_id', 'location_ascii_name']]\n",
    "        loc_df = loc_df.rename(columns={'location_id':'parent_id',\n",
    "                                        'location_ascii_name':'Country/Region'})\n",
    "        euro_df = euro_df.merge(loc_df)\n",
    "\n",
    "        euro_df = euro_df.loc[:, ['location_id', 'Location', 'Country/Region', 'level']]\n",
    "        return euro_df # don't like the name but probably easier to NOT change it.\n",
    "\n",
    "    loc_df = get_locations(LOCATION_SET_VERSION)\n",
    "    input_full_df = filter_data(inputs.load(MEASURES.full_data))\n",
    "    input_death_df = filter_data(inputs.load(MEASURES.deaths), kind='deaths')\n",
    "\n",
    "    # Subset to just dates for validation run\n",
    "    input_full_df = input_full_df[input_full_df['Date'] <= VALIDATION_DATE]\n",
    "    input_death_df = input_death_df[input_death_df['Date'] <= VALIDATION_DATE]\n",
    "\n",
    "    input_age_pop_df = inputs.load(MEASURES.age_pop)\n",
    "    input_age_death_df = inputs.load(MEASURES.age_death)\n",
    "    smoothed_case_df, smoothed_death_df = runner.get_smoothed(input_full_df)\n",
    "\n",
    "    # save cases for viz\n",
    "    smoothed_case_df[[COLUMNS.location_id, COLUMNS.date, 'ln(case rate)', 'population']].to_csv(\n",
    "        f'{OUTPUT_DIR}/smoothed_cases.csv', index=False\n",
    "    )\n",
    "\n",
    "    # Save pops for Bobby.\n",
    "    pop_df = input_age_pop_df.merge(loc_df).reset_index(drop=True)\n",
    "    pop_df[['location_id', 'Location', 'age_group', 'population']].to_csv(f'{OUTPUT_DIR}/pops.csv', index=False)\n",
    "\n",
    "    checkpoint.write('location', loc_df)\n",
    "    checkpoint.write('full_data', input_full_df)\n",
    "    checkpoint.write('deaths', input_death_df)\n",
    "    checkpoint.write('smoothed_cases', smoothed_case_df)\n",
    "    checkpoint.write('smoothed_deaths', smoothed_death_df)\n",
    "    checkpoint.write('age_pop', input_age_pop_df)\n",
    "    checkpoint.write('age_death', input_age_death_df)\n",
    "\n",
    "    #%%time\n",
    "    full_df = checkpoint.load('full_data')\n",
    "    death_df = checkpoint.load('deaths')\n",
    "    age_pop_df = checkpoint.load('age_pop')\n",
    "    age_death_df = checkpoint.load('age_death')\n",
    "\n",
    "    backcast_location_ids = runner.get_backcast_location_ids(full_df, most_detailed=False)\n",
    "    cases_and_backcast_deaths_df = runner.make_cases_and_backcast_deaths(full_df, death_df, age_pop_df, \n",
    "                                                                         age_death_df, backcast_location_ids, \n",
    "                                                                         subnat=False)\n",
    "\n",
    "    cases_and_backcast_deaths_df.to_csv(f'{OUTPUT_DIR}/backcast_for_case_to_death.csv', index=False)\n",
    "    checkpoint.write('cases_and_backcast_deaths', cases_and_backcast_deaths_df)\n",
    "\n",
    "    #%%time\n",
    "    cases_and_backcast_deaths_df = checkpoint.load('cases_and_backcast_deaths')\n",
    "    loc_df = checkpoint.load('location')\n",
    "\n",
    "    # loc_df needs to be updated with locations that have death data\n",
    "    # must have > 0/NaN deaths , seems to also need case data\n",
    "    model_run_locs = cases_and_backcast_deaths_df[cases_and_backcast_deaths_df['Deaths'].notnull() & cases_and_backcast_deaths_df['Confirmed case rate'].notnull()]\n",
    "    model_run_locs.to_csv(f'{OUTPUT_DIR}/model_locations.csv', index=False)\n",
    "    model_run_locs = model_run_locs['location_id'].unique()\n",
    "    \n",
    "    # An alternative for locations that survive cases_and_backcast_deaths at all\n",
    "    # model_run_locs = cases_and_backcast_deaths_df['location_id'].unique()\n",
    "    # model_run_locs.to_csv(f'{OUTPUT_DIR}/model_locations.csv', index=False)\n",
    "    # model_run_locs = model_run_locs['location_id'].unique()\n",
    "    # model_run_locs\n",
    "    \n",
    "    cases_and_backcast_deaths_df = cases_and_backcast_deaths_df[cases_and_backcast_deaths_df['location_id'].isin(model_run_locs)]\n",
    "    loc_df = loc_df[loc_df['location_id'].isin(model_run_locs)]\n",
    "\n",
    "    checkpoint.write('location', loc_df)\n",
    "\n",
    "    threshold_dates = runner.impute_death_threshold(cases_and_backcast_deaths_df,\n",
    "                                                    loc_df)\n",
    "    threshold_dates.to_csv(f'{OUTPUT_DIR}/threshold_dates.csv', index=False)\n",
    "    checkpoint.write('threshold_dates', threshold_dates)\n",
    "\n",
    "    smoothed_death_df = checkpoint.load('smoothed_deaths')\n",
    "    threshold_dates = checkpoint.load('threshold_dates')\n",
    "\n",
    "    date_mean_df = runner.make_date_mean_df(threshold_dates)\n",
    "    last_day_df = runner.make_last_day_df(smoothed_death_df,date_mean_df)\n",
    "    last_day_df.to_csv(f'{OUTPUT_DIR}/last_day.csv', index=False)\n",
    "\n",
    "    # this seems to be where some states are lost. Save these two files to investigate\n",
    "    smoothed_death_df.to_csv(f'{OUTPUT_DIR}/smoothed_death_df.csv', index=False)\n",
    "    date_mean_df.to_csv(f'{OUTPUT_DIR}/date_mean_df.csv', index=False)\n",
    "\n",
    "    checkpoint.write('date_mean', date_mean_df)\n",
    "    checkpoint.write('last_day', last_day_df)\n",
    "\n",
    "    full_df = checkpoint.load('full_data')\n",
    "    loc_df = checkpoint.load('location')\n",
    "    \n",
    "    # Keep locations we think will work (shouldn't be necessary here)\n",
    "    # loc_df = loc_df[loc_df['location_id'].isin(model_run_locs)]\n",
    "\n",
    "    df_to_run = full_df.loc[full_df[COLUMNS.location_id].isin(loc_df[COLUMNS.location_id].to_list())]\n",
    "    dcr_df, dhr_df, leading_indicator_df = runner.make_leading_indicator(\n",
    "        df_to_run,\n",
    "        SNAPSHOT_VERSION\n",
    "    )\n",
    "    dcr_df.to_csv(f'{OUTPUT_DIR}/lagged_death_to_case_ratios.csv', index=False)\n",
    "    dhr_df.to_csv(f'{OUTPUT_DIR}/lagged_death_to_hosp_ratios.csv', index=False)\n",
    "    leading_indicator_df.to_csv(f'{OUTPUT_DIR}/leading_indicator.csv', index=False)\n",
    "    leading_indicator_df = leading_indicator_df[[COLUMNS.location_id, COLUMNS.date, COLUMNS.ln_age_death_rate]]\n",
    "    leading_indicator_df = leading_indicator_df.loc[~leading_indicator_df[COLUMNS.ln_age_death_rate].isnull()]\n",
    "\n",
    "    checkpoint.write('leading_indicator', leading_indicator_df)\n",
    "\n",
    "    full_df = checkpoint.load('full_data')\n",
    "    death_df = checkpoint.load('deaths')\n",
    "    age_pop_df = checkpoint.load('age_pop')\n",
    "    age_death_df = checkpoint.load('age_death')\n",
    "    date_mean_df = checkpoint.load('date_mean')\n",
    "    last_day_df = checkpoint.load('last_day')\n",
    "    leading_indicator_df = checkpoint.load('leading_indicator')\n",
    "    loc_df = checkpoint.load('location')\n",
    "\n",
    "    #loc_df = loc_df[loc_df['location_id'].isin([3539, 60886, 60887])] # locations that didn't make it in round 1, but to finish all locs should be run\n",
    "    #loc_df = loc_df[loc_df['location_id'].isin([523, 530, 535, 556, 555, 533])]\n",
    "    \n",
    "    submodel_dict = runner.submit_models(death_df, age_pop_df, age_death_df, date_mean_df, leading_indicator_df,\n",
    "                                         loc_df, r0_locs,\n",
    "                                         PEAK_FILE, OUTPUT_DIR, \n",
    "                                         SNAPSHOT_VERSION, MODEL_INPUTS_VERSION, \n",
    "                                         R0_FILE, CODE_DIR, NO_PSEUDO)\n",
    "\n",
    "    checkpoint.write('submodel_dict', submodel_dict)\n",
    "\n",
    "    # Something from Mark, holds until jobs finish\n",
    "    import subprocess as sub\n",
    "    import time as time\n",
    "\n",
    "    length = sub.getoutput('qstat').count('curve')\n",
    "    while length != 0:\n",
    "        time.sleep(30)\n",
    "        length = sub.getoutput('qstat').count('curve')\n",
    "        print(length)\n",
    "    \n",
    "    smoothed_death_df = checkpoint.load('smoothed_deaths')\n",
    "    age_pop_df = checkpoint.load('age_pop')\n",
    "    threshold_dates = checkpoint.load('threshold_dates')\n",
    "    submodel_dict = checkpoint.load('submodel_dict')\n",
    "    loc_df = checkpoint.load('location')\n",
    "\n",
    "    # obs_df = full_df[full_df.location_id.isin(loc_df.location_id)]\n",
    "    obs_df = smoothed_death_df[smoothed_death_df.location_id.isin(loc_df.location_id)]\n",
    "\n",
    "    draw_dfs, past_draw_dfs, models_used, days, ensemble_draws_dfs, failed_locs = runner.compile_draws(\n",
    "        loc_df, submodel_dict, obs_df, threshold_dates, age_pop_df\n",
    "    )\n",
    "\n",
    "    if 'location' not in models_used:\n",
    "        raise ValueError('No location-specific draws used, must be using wrong tag')\n",
    "    draw_df = pd.concat(draw_dfs)\n",
    "    loc_df = loc_df.loc[~loc_df.location_id.isin(failed_locs)]\n",
    "    failed_df = loc_df.loc[loc_df.location_id.isin(failed_locs)]\n",
    "    failed_df.to_csv(f'{OUTPUT_DIR}/failed_curvefit_locations.csv', index=False)\n",
    "    \n",
    "    model_type_df = pd.DataFrame({'location': loc_df['Location'].tolist(),\n",
    "                                  'model_used': models_used})\n",
    "\n",
    "    # write\n",
    "    draw_df.to_csv(smooth_draw_path, index=False)\n",
    "    model_type_df.to_csv(f'{OUTPUT_DIR}/state_models_used.csv', index=False)\n",
    "#     ensemble_plot_path = runner.make_and_save_draw_plots(OUTPUT_DIR, loc_df,\n",
    "#                                                          ensemble_draws_dfs, days, models_used, age_pop_df)\n",
    "#     print(ensemble_plot_path)\n",
    "    checkpoint.write('draw_data', draw_df)\n",
    "    checkpoint.write('failed_locations', failed_locs)\n",
    "\n",
    "    raw_df = checkpoint.load('full_data')\n",
    "    loc_df = checkpoint.load('location')\n",
    "    loc_df = loc_df.loc[~loc_df.location_id.isin(failed_locs)]\n",
    "    raw_df['Location'] = raw_df['Province/State']\n",
    "    raw_df = raw_df.loc[raw_df['location_id'].isin(loc_df['location_id'].to_list())]\n",
    "    raw_df.loc[raw_df['Location'].isnull(), 'Location'] = raw_df['Country/Region']\n",
    "    runner.swap_observed(OUTPUT_DIR, smooth_draw_path, raw_draw_path, raw_df)\n",
    "\n",
    "# I don't know what this does   \n",
    "#     loc_df = checkpoint.load('location')\n",
    "#     submodel_dict = checkpoint.load('submodel_dict')\n",
    "#     draw_df = checkpoint.load('draw_data')\n",
    "#     age_pop_df = checkpoint.load('age_pop')\n",
    "#     runner.save_points_and_peaks(loc_df, submodel_dict, draw_df, age_pop_df, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  523,   525,   526,   527,   528,   529,   530,   531,   532,\n",
       "         533,   535,   536,   537,   538,   539,   540,   541,   543,\n",
       "         544,   545,   546,   547,   548,   551,   553,   555,   556,\n",
       "         558,   559,   560,   561,   563,   565,   566,   568,   569,\n",
       "         572,  4651,  4766,  4768,  4775, 35494, 35495, 35496, 35497,\n",
       "       35498, 35499, 35500, 35501, 35502, 35503, 35504, 35505, 35506,\n",
       "       35507, 35508, 35509, 35510, 35511, 35512, 35513, 35514, 60412,\n",
       "       60377, 60378, 60379, 60383, 60384, 60386, 60387, 60390, 60391,\n",
       "       60392, 60357, 60358, 60359, 60360, 60361, 60362, 60363, 60364,\n",
       "       60365, 60366, 60367, 60368, 60370, 60371, 60374, 60376, 60372,\n",
       "       43859, 43866, 43868, 60886, 60887])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cases_and_backcast_deaths_df[cases_and_backcast_deaths_df['location_id']==62].tail(10)\n",
    "model_run_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VALIDATION_DATE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-524c23d8b1ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mVALIDATION_FOLDER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVALIDATION_DATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mCODE_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../src/covid_model_deaths'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}/{VALIDATION_FOLDER}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VALIDATION_DATE' is not defined"
     ]
    }
   ],
   "source": [
    "VALIDATION_FOLDER = re.sub(\"-\", \"_\", VALIDATION_DATE)\n",
    "CODE_DIR = os.path.abspath('../src/covid_model_deaths')\n",
    "OUTPUT_DIR = f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}/{VALIDATION_FOLDER}'\n",
    "if not os.path.exists(f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}'):\n",
    "    os.mkdir(f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}')\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)\n",
    "inputs = InputsContext(f'/ihme/covid-19/model-inputs/{MODEL_INPUTS_VERSION}')\n",
    "checkpoint = Checkpoint(OUTPUT_DIR)\n",
    "    \n",
    "full_df = checkpoint.load('full_data')\n",
    "death_df = checkpoint.load('deaths')\n",
    "age_pop_df = checkpoint.load('age_pop')\n",
    "age_death_df = checkpoint.load('age_death')\n",
    "date_mean_df = checkpoint.load('date_mean')\n",
    "last_day_df = checkpoint.load('last_day')\n",
    "leading_indicator_df = checkpoint.load('leading_indicator')\n",
    "loc_df = checkpoint.load('location')\n",
    "\n",
    "loc_df = loc_df[loc_df['location_id'].isin([3539])]\n",
    "\n",
    "submodel_dict = runner.submit_models(death_df, age_pop_df, age_death_df, date_mean_df, leading_indicator_df,\n",
    "                                     loc_df, r0_locs,\n",
    "                                     PEAK_FILE, OUTPUT_DIR, \n",
    "                                     SNAPSHOT_VERSION, MODEL_INPUTS_VERSION, \n",
    "                                     R0_FILE, CODE_DIR, NO_PSEUDO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>Location</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>60886</td>\n",
       "      <td>King and Snohomish Counties</td>\n",
       "      <td>Washington</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>3539</td>\n",
       "      <td>Spokane County</td>\n",
       "      <td>Washington</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>60887</td>\n",
       "      <td>Washington except for King, Snohomish, and Spo...</td>\n",
       "      <td>Washington</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     location_id                                           Location  \\\n",
       "132        60886                        King and Snohomish Counties   \n",
       "133         3539                                     Spokane County   \n",
       "134        60887  Washington except for King, Snohomish, and Spo...   \n",
       "\n",
       "    Country/Region  level  \n",
       "132     Washington      2  \n",
       "133     Washington      2  \n",
       "134     Washington      2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_df.head()\n",
    "loc_df = loc_df[loc_df['location_id'].isin([3539, 60886, 60887])]\n",
    "loc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>population</th>\n",
       "      <th>Confirmed case rate</th>\n",
       "      <th>Death rate</th>\n",
       "      <th>Hospitalizations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-503</td>\n",
       "      <td>Outside Hubei</td>\n",
       "      <td>China</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.366871e+09</td>\n",
       "      <td>7.608620e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Taiwan (Province of China)</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.362024e+07</td>\n",
       "      <td>4.233657e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cambodia</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.660312e+07</td>\n",
       "      <td>6.022965e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.594658e+08</td>\n",
       "      <td>7.708144e-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lao People's Democratic Republic</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.158250e+06</td>\n",
       "      <td>2.793979e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35497</th>\n",
       "      <td>43869</td>\n",
       "      <td>Saskatchewan</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.138791e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35577</th>\n",
       "      <td>43870</td>\n",
       "      <td>Yukon</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.958754e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35657</th>\n",
       "      <td>3539</td>\n",
       "      <td>Spokane County</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.152500e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35774</th>\n",
       "      <td>60886</td>\n",
       "      <td>King and Snohomish Counties</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.045000e+06</td>\n",
       "      <td>1.313629e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35891</th>\n",
       "      <td>60887</td>\n",
       "      <td>Washington except for King, Snohomish, and Spo...</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.741327e+06</td>\n",
       "      <td>2.672848e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>405 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       location_id                                     Province/State  \\\n",
       "0             -503                                      Outside Hubei   \n",
       "129              8                                                NaN   \n",
       "258             10                                                NaN   \n",
       "382             11                                                NaN   \n",
       "471             12                                                NaN   \n",
       "...            ...                                                ...   \n",
       "35497        43869                                       Saskatchewan   \n",
       "35577        43870                                              Yukon   \n",
       "35657         3539                                     Spokane County   \n",
       "35774        60886                        King and Snohomish Counties   \n",
       "35891        60887  Washington except for King, Snohomish, and Spo...   \n",
       "\n",
       "                         Country/Region       Date  Confirmed  Deaths  \\\n",
       "0                                 China 2020-01-22      104.0     0.0   \n",
       "129          Taiwan (Province of China) 2020-01-22        1.0     0.0   \n",
       "258                            Cambodia 2020-01-27        1.0     0.0   \n",
       "382                           Indonesia 2020-03-02        2.0     0.0   \n",
       "471    Lao People's Democratic Republic 2020-03-24        2.0     0.0   \n",
       "...                                 ...        ...        ...     ...   \n",
       "35497                            Canada 2020-03-11        0.0     0.0   \n",
       "35577                            Canada 2020-03-11        0.0     0.0   \n",
       "35657          United States of America 2020-02-02        0.0     0.0   \n",
       "35774          United States of America 2020-02-02        4.0     0.0   \n",
       "35891          United States of America 2020-02-02        1.0     9.0   \n",
       "\n",
       "         population  Confirmed case rate  Death rate  Hospitalizations  \n",
       "0      1.366871e+09         7.608620e-08    0.000000               NaN  \n",
       "129    2.362024e+07         4.233657e-08    0.000000               NaN  \n",
       "258    1.660312e+07         6.022965e-08    0.000000               NaN  \n",
       "382    2.594658e+08         7.708144e-09    0.000000               NaN  \n",
       "471    7.158250e+06         2.793979e-07    0.000000               NaN  \n",
       "...             ...                  ...         ...               ...  \n",
       "35497  1.138791e+06         0.000000e+00    0.000000               NaN  \n",
       "35577  3.958754e+04         0.000000e+00    0.000000               NaN  \n",
       "35657  5.152500e+05         0.000000e+00    0.000000               NaN  \n",
       "35774  3.045000e+06         1.313629e-06    0.000000               NaN  \n",
       "35891  3.741327e+06         2.672848e-07    0.000002               NaN  \n",
       "\n",
       "[405 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()\n",
    "full_df.groupby('location_id').head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2134defb4daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdate_mean_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{OUTPUT_DIR}/date_mean_df.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "date_mean_df = runner.make_date_mean_df(threshold_dates)\n",
    "last_day_df = runner.make_last_day_df(smoothed_death_df,date_mean_df)\n",
    "last_day_df.to_csv(f'{OUTPUT_DIR}/last_day.csv', index=False)\n",
    "\n",
    "# this seems to be where some states are lost.\n",
    "# smoothed_death_df.to_csv(f'{OUTPUT_DIR}/smoothed_death_df.csv', index=False)\n",
    "date_mean_df.to_csv(f'{OUTPUT_DIR}/date_mean_df.csv', index=False)\n",
    "\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
