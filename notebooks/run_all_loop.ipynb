{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to run Curvefit across all locations for model validation exercise\n",
    "## All models will be run in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "from db_queries import get_location_metadata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "from covid_model_deaths import runner\n",
    "from covid_model_deaths.deaths_io import InputsContext, MEASURES, Checkpoint\n",
    "from covid_model_deaths.globals import COLUMNS\n",
    "\n",
    "pd.options.display.max_rows = 99\n",
    "pd.options.display.max_columns = 99\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set dates for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DATE = \"2020_06_26\" # Date for all data used\n",
    "# Completed these \"2020-03-27\", \"2020-04-03\", \"2020-04-10\",\n",
    "# \"2020-04-17\" breaks, do the rest?\n",
    "# date_list = [\"2020-04-24\",\"2020-05-01\",\"2020-05-08\",\"2020-05-15\",\"2020-05-22\",\"2020-05-29\"]\n",
    "# date_list = [\"2020-03-27\", \"2020-04-03\", \"2020-04-10\", \"2020-04-17\"]\n",
    "date_list = [\"2020-04-24\",\"2020-05-01\",\"2020-05-08\",\"2020-05-15\",\"2020-05-22\",\"2020-05-29\"]\n",
    "date_list = [\"2020-03-31\",\"2020-03-31\",\"2020-04-03\", \"2020-04-10\", \"2020-04-17\",\"2020-04-24\",\"2020-05-01\"]\n",
    "date_list = [\"2020-03-27\",\"2020-03-31\",\"2020-04-03\", \"2020-04-10\", \"2020-04-17\",\"2020-04-24\",\"2020-05-01\"]\n",
    "date_list = [\"2020-04-24\",\"2020-05-01\",\"2020-05-08\",\"2020-05-15\"]\n",
    "date_list = [\"2020-05-22\",\"2020-05-29\",\"2020-06-05\",\"2020-06-12\",\"2020-06-19\"]\n",
    "date_list = [\"2020-04-24\",\"2020-05-01\",\"2020-05-04\",\"2020-05-08\",\"2020-05-15\",\"2020-06-01\",\"2020-06-26\"]\n",
    "# \"2020-03-27\", \"2020-04-17\", breaks ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-30 16:12:41.949 | DEBUG    | covid_model_deaths.deaths_io.checkpoint:_setup_checkpoint_dir:45 - Making checkpoint directory at /ihme/covid-19/deaths/validation/2020_06_26/2020_04_24/checkpoint\n",
      "2020-06-30 16:12:41.988 | DEBUG    | covid_model_deaths.deaths_io.inputs:load:40 - Loading full_data.csv from /ihme/covid-19/model-inputs/2020_06_26.01.\n",
      "2020-06-30 16:12:42.063 | DEBUG    | covid_model_deaths.deaths_io.inputs:load:40 - Loading deaths.csv from /ihme/covid-19/model-inputs/2020_06_26.01.\n",
      "2020-06-30 16:12:42.113 | DEBUG    | covid_model_deaths.deaths_io.inputs:load:40 - Loading age_pop.csv from /ihme/covid-19/model-inputs/2020_06_26.01.\n",
      "2020-06-30 16:12:42.124 | DEBUG    | covid_model_deaths.deaths_io.inputs:load:40 - Loading age_death.csv from /ihme/covid-19/model-inputs/2020_06_26.01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /ihme/covid-19/deaths/validation/2020_06_26/2020_04_24\n",
      "/ihme/code/covid-19/user/ctroeger/covid-model-deaths/src/covid_model_deaths\n",
      "Checkpoint(/ihme/covid-19/deaths/validation/2020_06_26/2020_04_24/checkpoint)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-30 16:12:56.424 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading full_data from in memory cache.\n",
      "2020-06-30 16:12:56.426 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading deaths from in memory cache.\n",
      "2020-06-30 16:12:56.427 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading age_pop from in memory cache.\n",
      "2020-06-30 16:12:56.428 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading age_death from in memory cache.\n",
      " 12%|█▏        | 40/324 [00:25<07:53,  1.67s/it] "
     ]
    }
   ],
   "source": [
    "for day in date_list:\n",
    "    VALIDATION_DATE = day # Date to use data upto and including\n",
    "\n",
    "    RUN_TYPE = 'validation'\n",
    "    MODEL_INPUTS_VERSION = 'production-runs/' + DATA_DATE\n",
    "    SNAPSHOT_VERSION = 'production-runs/' + DATA_DATE\n",
    "    DATESTAMP_LABEL = '2020_05_23_Europe' # Will want to change this.\n",
    "\n",
    "    PEAK_FILE = '/ihme/covid-19/deaths/mobility_inputs/2020_04_20/peak_locs_april20_.csv'\n",
    "    PEAK_DURATION_FILE = None\n",
    "    R0_FILE = None\n",
    "    LOCATION_SET_VERSION = 720\n",
    "    r0_locs = []\n",
    "    # Locations where no pseudo data is used\n",
    "    NO_PSEUDO = [\n",
    "        564, # South Dakota\n",
    "        538, # Iowa\n",
    "        # Mexican subnationals\n",
    "        4644, 4657, 4651, 4663, 4665, 4667, 4669\n",
    "    ]\n",
    "\n",
    "    VALIDATION_FOLDER = re.sub(\"-\", \"_\", VALIDATION_DATE)\n",
    "    CODE_DIR = os.path.abspath('../src/covid_model_deaths')\n",
    "    OUTPUT_DIR = f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}/{VALIDATION_FOLDER}'\n",
    "    if not os.path.exists(f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}'):\n",
    "        os.mkdir(f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}')\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.mkdir(OUTPUT_DIR)\n",
    "    inputs = InputsContext(f'/ihme/covid-19/model-inputs/{MODEL_INPUTS_VERSION}')\n",
    "    checkpoint = Checkpoint(OUTPUT_DIR)\n",
    "\n",
    "    smooth_draw_path = f'{OUTPUT_DIR}/smoothed_euro_data.csv'\n",
    "    raw_draw_path = f'{OUTPUT_DIR}/euro_data.csv'\n",
    "    average_draw_path = f'{OUTPUT_DIR}/past_avg_smoothed_euro_data.csv'\n",
    "    yesterday_draw_path = '/ihme/covid-19/deaths/prod/2020_05_22_Europe/smoothed_euro_data.csv'\n",
    "    before_yesterday_draw_path = '/ihme/covid-19/deaths/prod/2020_05_19_Europe/smoothed_euro_data.csv'\n",
    "    compare_average_path = '/ihme/covid-19/deaths/prod/2020_05_22_Europe/smoothed_euro_data.csv'\n",
    "\n",
    "    print(f'Writing to {OUTPUT_DIR}')\n",
    "    print(CODE_DIR)\n",
    "    print(checkpoint)\n",
    "\n",
    "    metadata = {}\n",
    "    with open(f'/ihme/covid-19/model-inputs/{MODEL_INPUTS_VERSION}/metadata.yaml') as f:\n",
    "        metadata['inputs_version'] = yaml.full_load(f)\n",
    "\n",
    "    metadata['run_type'] = RUN_TYPE\n",
    "    metadata['model_inputs_version'] = MODEL_INPUTS_VERSION\n",
    "    metadata['snapshot_version'] = SNAPSHOT_VERSION\n",
    "    metadata['datestamp_label'] = DATESTAMP_LABEL\n",
    "    metadata['peak_file'] = PEAK_FILE\n",
    "    metadata['location_set_version_id'] = LOCATION_SET_VERSION\n",
    "    metadata['output_dir'] = OUTPUT_DIR\n",
    "    metadata['no_pseudo'] = NO_PSEUDO\n",
    "    metadata['average'] = {\n",
    "        'yesterday': yesterday_draw_path,\n",
    "        'before_yesterday': before_yesterday_draw_path\n",
    "    }\n",
    "    metadata['compare_average'] = compare_average_path\n",
    "\n",
    "    with open(f'{OUTPUT_DIR}/metadata.yaml', 'w') as f:\n",
    "        yaml.dump(metadata, f)\n",
    "\n",
    "    def filter_data(data: pd.DataFrame, kind='full') -> pd.DataFrame:\n",
    "        # manually adjust Iceland spike (0 deaths to 5 deaths to 0 deaths in March...)\n",
    "        iceland = data['Country/Region'] == 'Iceland'\n",
    "        iceland_spike = iceland & (data['Date'] == pd.Timestamp('2020-03-15'))\n",
    "        if kind == 'full':\n",
    "            data.loc[iceland_spike, ['Deaths', 'Death rate']] = 0\n",
    "        elif kind == 'deaths':\n",
    "            data = data.loc[~iceland_spike]\n",
    "            min_iceland_date = data.loc[iceland, 'Date'].min()\n",
    "            data.loc[iceland, 'Days'] = (data.loc[iceland, 'Date'] - min_iceland_date).dt.days\n",
    "\n",
    "        catalonia  = data['location_id'] == 60368\n",
    "        catalonia_spike = catalonia & (data['Date'] >= pd.Timestamp('2020-05-21'))\n",
    "        data = data[~catalonia_spike]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def get_locations(location_set_version_id):\n",
    "        # get locaton_ids\n",
    "        loc_df = get_location_metadata(location_set_id=111,\n",
    "                                       location_set_version_id=location_set_version_id)\n",
    "\n",
    "        # Drop any locations in the US and keep only most detailed for modeling\n",
    "        most_detailed = loc_df['most_detailed'] == 1\n",
    "        # non_us = ~loc_df['path_to_top_parent'].str.startswith('102,')\n",
    "        keep_columns = ['location_id', 'location_ascii_name', 'parent_id', 'level', 'most_detailed']\n",
    "\n",
    "        euro_df = loc_df.loc[most_detailed, keep_columns]\n",
    "        euro_df = euro_df.rename(columns={'location_ascii_name':'Location'})\n",
    "\n",
    "        # Add parents\n",
    "        loc_df = loc_df[['location_id', 'location_ascii_name']]\n",
    "        loc_df = loc_df.rename(columns={'location_id':'parent_id',\n",
    "                                        'location_ascii_name':'Country/Region'})\n",
    "        euro_df = euro_df.merge(loc_df)\n",
    "\n",
    "        euro_df = euro_df.loc[:, ['location_id', 'Location', 'Country/Region', 'level']]\n",
    "        return euro_df # don't like the name but probably easier to NOT change it.\n",
    "\n",
    "    loc_df = get_locations(LOCATION_SET_VERSION)\n",
    "    input_full_df = filter_data(inputs.load(MEASURES.full_data))\n",
    "    input_death_df = filter_data(inputs.load(MEASURES.deaths), kind='deaths')\n",
    "\n",
    "    # Subset to just dates for validation run\n",
    "    input_full_df = input_full_df[input_full_df['Date'] <= VALIDATION_DATE]\n",
    "    input_death_df = input_death_df[input_death_df['Date'] <= VALIDATION_DATE]\n",
    "\n",
    "    input_age_pop_df = inputs.load(MEASURES.age_pop)\n",
    "    input_age_death_df = inputs.load(MEASURES.age_death)\n",
    "    smoothed_case_df, smoothed_death_df = runner.get_smoothed(input_full_df)\n",
    "\n",
    "    # save cases for viz\n",
    "    smoothed_case_df[[COLUMNS.location_id, COLUMNS.date, 'ln(case rate)', 'population']].to_csv(\n",
    "        f'{OUTPUT_DIR}/smoothed_cases.csv', index=False\n",
    "    )\n",
    "\n",
    "    # Save pops for Bobby.\n",
    "    pop_df = input_age_pop_df.merge(loc_df).reset_index(drop=True)\n",
    "    pop_df[['location_id', 'Location', 'age_group', 'population']].to_csv(f'{OUTPUT_DIR}/pops.csv', index=False)\n",
    "\n",
    "    checkpoint.write('location', loc_df)\n",
    "    checkpoint.write('full_data', input_full_df)\n",
    "    checkpoint.write('deaths', input_death_df)\n",
    "    checkpoint.write('smoothed_cases', smoothed_case_df)\n",
    "    checkpoint.write('smoothed_deaths', smoothed_death_df)\n",
    "    checkpoint.write('age_pop', input_age_pop_df)\n",
    "    checkpoint.write('age_death', input_age_death_df)\n",
    "\n",
    "    #%%time\n",
    "    full_df = checkpoint.load('full_data')\n",
    "    death_df = checkpoint.load('deaths')\n",
    "    age_pop_df = checkpoint.load('age_pop')\n",
    "    age_death_df = checkpoint.load('age_death')\n",
    "\n",
    "    full_df.to_csv(f'{OUTPUT_DIR}/full_df.csv', index=False)\n",
    "    death_df.to_csv(f'{OUTPUT_DIR}/death_df.csv', index=False)\n",
    "    \n",
    "    backcast_location_ids = runner.get_backcast_location_ids(full_df, most_detailed=False)\n",
    "    cases_and_backcast_deaths_df = runner.make_cases_and_backcast_deaths(full_df, death_df, age_pop_df, \n",
    "                                                                         age_death_df, backcast_location_ids, \n",
    "                                                                         subnat=False)\n",
    "\n",
    "    cases_and_backcast_deaths_df.to_csv(f'{OUTPUT_DIR}/backcast_for_case_to_death.csv', index=False)\n",
    "    checkpoint.write('cases_and_backcast_deaths', cases_and_backcast_deaths_df)\n",
    "    \n",
    "    #%%time\n",
    "    cases_and_backcast_deaths_df = checkpoint.load('cases_and_backcast_deaths')\n",
    "    loc_df = checkpoint.load('location')\n",
    "\n",
    "    # loc_df needs to be updated with locations that have death data\n",
    "    # must have > 0/NaN deaths , seems to also need case data\n",
    "    #model_run_locs = cases_and_backcast_deaths_df[cases_and_backcast_deaths_df['Deaths'].notnull() & cases_and_backcast_deaths_df['Confirmed case rate'].notnull()]\n",
    "    #model_run_locs = cases_and_backcast_deaths_df\n",
    "    df = cases_and_backcast_deaths_df\n",
    "    \n",
    "    # Let's write this as a CSV and interrogate in R\n",
    "    df.to_csv(f'{OUTPUT_DIR}/case_backcast_deaths.csv', index=False)\n",
    "    \n",
    "    df = df[df['location_id'] > 0]\n",
    "    df = df[df['Confirmed case rate'] > 0]\n",
    "    t = df.groupby(df['location_id']).mean().dropna()\n",
    "    t = df.groupby('location_id')['Confirmed case rate'].mean().dropna()\n",
    "    model_run_locs = t.index.values\n",
    "    model_run_locs = pd.DataFrame(model_run_locs, columns = ['location_id'])\n",
    "    \n",
    "    model_run_locs.to_csv(f'{OUTPUT_DIR}/model_locations.csv', index=False)\n",
    "    model_run_locs = model_run_locs['location_id'].unique()\n",
    "\n",
    "    # An alternative for locations that survive cases_and_backcast_deaths at all\n",
    "    # model_run_locs = cases_and_backcast_deaths_df['location_id'].unique()\n",
    "    # model_run_locs.to_csv(f'{OUTPUT_DIR}/model_locations.csv', index=False)\n",
    "    # model_run_locs = model_run_locs['location_id'].unique()\n",
    "    # model_run_locs\n",
    "    \n",
    "    cases_and_backcast_deaths_df = cases_and_backcast_deaths_df[cases_and_backcast_deaths_df['location_id'].isin(model_run_locs)]\n",
    "    loc_df = loc_df[loc_df['location_id'].isin(model_run_locs)]\n",
    "\n",
    "    checkpoint.write('location', loc_df)\n",
    "\n",
    "    threshold_dates = runner.impute_death_threshold(cases_and_backcast_deaths_df,\n",
    "                                                    loc_df)\n",
    "    threshold_dates.to_csv(f'{OUTPUT_DIR}/threshold_dates.csv', index=False)\n",
    "    checkpoint.write('threshold_dates', threshold_dates)\n",
    "\n",
    "    smoothed_death_df = checkpoint.load('smoothed_deaths')\n",
    "    threshold_dates = checkpoint.load('threshold_dates')\n",
    "\n",
    "    date_mean_df = runner.make_date_mean_df(threshold_dates)\n",
    "    last_day_df = runner.make_last_day_df(smoothed_death_df,date_mean_df)\n",
    "    last_day_df.to_csv(f'{OUTPUT_DIR}/last_day.csv', index=False)\n",
    "\n",
    "    # this seems to be where some states are lost. Save these two files to investigate\n",
    "    smoothed_death_df.to_csv(f'{OUTPUT_DIR}/smoothed_death_df.csv', index=False)\n",
    "    date_mean_df.to_csv(f'{OUTPUT_DIR}/date_mean_df.csv', index=False)\n",
    "\n",
    "    checkpoint.write('date_mean', date_mean_df)\n",
    "    checkpoint.write('last_day', last_day_df)\n",
    "\n",
    "    full_df = checkpoint.load('full_data')\n",
    "    loc_df = checkpoint.load('location')\n",
    "    \n",
    "    # Keep locations we think will work (shouldn't be necessary here)\n",
    "    #loc_df = loc_df[loc_df['location_id'].isin(model_run_locs)]\n",
    "\n",
    "    df_to_run = full_df.loc[full_df[COLUMNS.location_id].isin(loc_df[COLUMNS.location_id].to_list())]\n",
    "    dcr_df, dhr_df, leading_indicator_df = runner.make_leading_indicator(\n",
    "        df_to_run,\n",
    "        SNAPSHOT_VERSION\n",
    "    )\n",
    "    dcr_df.to_csv(f'{OUTPUT_DIR}/lagged_death_to_case_ratios.csv', index=False)\n",
    "    dhr_df.to_csv(f'{OUTPUT_DIR}/lagged_death_to_hosp_ratios.csv', index=False)\n",
    "    leading_indicator_df.to_csv(f'{OUTPUT_DIR}/leading_indicator.csv', index=False)\n",
    "    leading_indicator_df = leading_indicator_df[[COLUMNS.location_id, COLUMNS.date, COLUMNS.ln_age_death_rate]]\n",
    "    leading_indicator_df = leading_indicator_df.loc[~leading_indicator_df[COLUMNS.ln_age_death_rate].isnull()]\n",
    "\n",
    "    checkpoint.write('leading_indicator', leading_indicator_df)\n",
    "\n",
    "    full_df = checkpoint.load('full_data')\n",
    "    death_df = checkpoint.load('deaths')\n",
    "    age_pop_df = checkpoint.load('age_pop')\n",
    "    age_death_df = checkpoint.load('age_death')\n",
    "    date_mean_df = checkpoint.load('date_mean')\n",
    "    last_day_df = checkpoint.load('last_day')\n",
    "    leading_indicator_df = checkpoint.load('leading_indicator')\n",
    "    loc_df = checkpoint.load('location')\n",
    "\n",
    "    #loc_df = loc_df[loc_df['location_id'].isin([3539, 60886, 60887])] # locations that didn't make it in round 1, but to finish all locs should be run\n",
    "    #loc_df = loc_df[loc_df['location_id'].isin([523, 530, 535, 556, 555, 533])]\n",
    "    \n",
    "    submodel_dict = runner.submit_models(death_df, age_pop_df, age_death_df, date_mean_df, leading_indicator_df,\n",
    "                                         loc_df, r0_locs,\n",
    "                                         PEAK_FILE, OUTPUT_DIR, \n",
    "                                         SNAPSHOT_VERSION, MODEL_INPUTS_VERSION, \n",
    "                                         R0_FILE, CODE_DIR, NO_PSEUDO)\n",
    "\n",
    "    checkpoint.write('submodel_dict', submodel_dict)\n",
    "\n",
    "    # Something from Mark, holds until jobs finish\n",
    "    import subprocess as sub\n",
    "    import time as time\n",
    "\n",
    "    length = sub.getoutput('qstat').count('curve')\n",
    "    while length != 0:\n",
    "        time.sleep(30)\n",
    "        length = sub.getoutput('qstat').count('curve')\n",
    "        print(length)\n",
    "    \n",
    "    smoothed_death_df = checkpoint.load('smoothed_deaths')\n",
    "    age_pop_df = checkpoint.load('age_pop')\n",
    "    threshold_dates = checkpoint.load('threshold_dates')\n",
    "    submodel_dict = checkpoint.load('submodel_dict')\n",
    "    loc_df = checkpoint.load('location')\n",
    "\n",
    "    # obs_df = full_df[full_df.location_id.isin(loc_df.location_id)]\n",
    "    obs_df = smoothed_death_df[smoothed_death_df.location_id.isin(loc_df.location_id)]\n",
    "\n",
    "    draw_dfs, past_draw_dfs, models_used, days, ensemble_draws_dfs, failed_locs = runner.compile_draws(\n",
    "        loc_df, submodel_dict, obs_df, threshold_dates, age_pop_df\n",
    "    )\n",
    "\n",
    "    if 'location' not in models_used:\n",
    "        raise ValueError('No location-specific draws used, must be using wrong tag')\n",
    "    draw_df = pd.concat(draw_dfs)\n",
    "    failed_df = loc_df.loc[loc_df.location_id.isin(failed_locs)]\n",
    "    loc_df = loc_df.loc[~loc_df.location_id.isin(failed_locs)]\n",
    "    \n",
    "    failed_df.to_csv(f'{OUTPUT_DIR}/failed_curvefit_locations.csv', index=False)\n",
    "    \n",
    "    model_type_df = pd.DataFrame({'location': loc_df['Location'].tolist(),\n",
    "                                  'model_used': models_used})\n",
    "\n",
    "    # write\n",
    "    draw_df.to_csv(smooth_draw_path, index=False)\n",
    "    model_type_df.to_csv(f'{OUTPUT_DIR}/state_models_used.csv', index=False)\n",
    "#     ensemble_plot_path = runner.make_and_save_draw_plots(OUTPUT_DIR, loc_df,\n",
    "#                                                          ensemble_draws_dfs, days, models_used, age_pop_df)\n",
    "#     print(ensemble_plot_path)\n",
    "    checkpoint.write('draw_data', draw_df)\n",
    "    checkpoint.write('failed_locations', failed_locs)\n",
    "\n",
    "    raw_df = checkpoint.load('full_data')\n",
    "    loc_df = checkpoint.load('location')\n",
    "    loc_df = loc_df.loc[~loc_df.location_id.isin(failed_locs)]\n",
    "    raw_df['Location'] = raw_df['Province/State']\n",
    "    raw_df = raw_df.loc[raw_df['location_id'].isin(loc_df['location_id'].to_list())]\n",
    "    raw_df.loc[raw_df['Location'].isnull(), 'Location'] = raw_df['Country/Region']\n",
    "    runner.swap_observed(OUTPUT_DIR, smooth_draw_path, raw_draw_path, raw_df)\n",
    "\n",
    "# I don't know what this does   \n",
    "#     loc_df = checkpoint.load('location')\n",
    "#     submodel_dict = checkpoint.load('submodel_dict')\n",
    "#     draw_df = checkpoint.load('draw_data')\n",
    "#     age_pop_df = checkpoint.load('age_pop')\n",
    "#     runner.save_points_and_peaks(loc_df, submodel_dict, draw_df, age_pop_df, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-15 12:08:01.749 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading full_data from in memory cache.\n",
      "2020-06-15 12:08:01.752 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading deaths from in memory cache.\n",
      "2020-06-15 12:08:01.753 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading age_pop from in memory cache.\n",
      "2020-06-15 12:08:01.754 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading age_death from in memory cache.\n",
      "2020-06-15 12:08:01.755 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading date_mean from in memory cache.\n",
      "2020-06-15 12:08:01.756 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading last_day from in memory cache.\n",
      "2020-06-15 12:08:01.757 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading leading_indicator from in memory cache.\n",
      "2020-06-15 12:08:01.759 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading location from in memory cache.\n",
      "100%|██████████| 2/2 [00:46<00:00, 23.05s/it]\n"
     ]
    }
   ],
   "source": [
    "#cases_and_backcast_deaths_df[cases_and_backcast_deaths_df['location_id']==62].tail(10)\n",
    "# full_df.head()\n",
    "# full_df[full_df['Country/Region'] == \"Sweden\"]\n",
    "\n",
    "full_df = checkpoint.load('full_data')\n",
    "death_df = checkpoint.load('deaths')\n",
    "age_pop_df = checkpoint.load('age_pop')\n",
    "age_death_df = checkpoint.load('age_death')\n",
    "date_mean_df = checkpoint.load('date_mean')\n",
    "last_day_df = checkpoint.load('last_day')\n",
    "leading_indicator_df = checkpoint.load('leading_indicator')\n",
    "loc_df = checkpoint.load('location')\n",
    "    \n",
    "loc_df = loc_df[loc_df['location_id'].isin([48, 52])]\n",
    "\n",
    "submodel_dict = runner.submit_models(death_df, age_pop_df, age_death_df, date_mean_df, leading_indicator_df,\n",
    "                                     loc_df, r0_locs,\n",
    "                                     PEAK_FILE, OUTPUT_DIR, \n",
    "                                     SNAPSHOT_VERSION, MODEL_INPUTS_VERSION, \n",
    "                                     R0_FILE, CODE_DIR, NO_PSEUDO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VALIDATION_DATE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-524c23d8b1ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mVALIDATION_FOLDER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVALIDATION_DATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mCODE_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../src/covid_model_deaths'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}/{VALIDATION_FOLDER}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VALIDATION_DATE' is not defined"
     ]
    }
   ],
   "source": [
    "VALIDATION_FOLDER = re.sub(\"-\", \"_\", VALIDATION_DATE)\n",
    "CODE_DIR = os.path.abspath('../src/covid_model_deaths')\n",
    "OUTPUT_DIR = f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}/{VALIDATION_FOLDER}'\n",
    "if not os.path.exists(f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}'):\n",
    "    os.mkdir(f'/ihme/covid-19/deaths/{RUN_TYPE}/{DATA_DATE}')\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)\n",
    "inputs = InputsContext(f'/ihme/covid-19/model-inputs/{MODEL_INPUTS_VERSION}')\n",
    "checkpoint = Checkpoint(OUTPUT_DIR)\n",
    "    \n",
    "full_df = checkpoint.load('full_data')\n",
    "death_df = checkpoint.load('deaths')\n",
    "age_pop_df = checkpoint.load('age_pop')\n",
    "age_death_df = checkpoint.load('age_death')\n",
    "date_mean_df = checkpoint.load('date_mean')\n",
    "last_day_df = checkpoint.load('last_day')\n",
    "leading_indicator_df = checkpoint.load('leading_indicator')\n",
    "loc_df = checkpoint.load('location')\n",
    "\n",
    "loc_df = loc_df[loc_df['location_id'].isin([3539])]\n",
    "\n",
    "submodel_dict = runner.submit_models(death_df, age_pop_df, age_death_df, date_mean_df, leading_indicator_df,\n",
    "                                     loc_df, r0_locs,\n",
    "                                     PEAK_FILE, OUTPUT_DIR, \n",
    "                                     SNAPSHOT_VERSION, MODEL_INPUTS_VERSION, \n",
    "                                     R0_FILE, CODE_DIR, NO_PSEUDO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-07 09:13:31.130 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading full_data from in memory cache.\n",
      "2020-06-07 09:13:31.132 | INFO     | covid_model_deaths.deaths_io.checkpoint:load:30 - Loading deaths from in memory cache.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region_x</th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths_x</th>\n",
       "      <th>population_x</th>\n",
       "      <th>Confirmed case rate</th>\n",
       "      <th>Death rate_x</th>\n",
       "      <th>Hospitalizations</th>\n",
       "      <th>Location</th>\n",
       "      <th>Country/Region_y</th>\n",
       "      <th>Days</th>\n",
       "      <th>Deaths_y</th>\n",
       "      <th>Death rate_y</th>\n",
       "      <th>population_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Norway</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.348847e+06</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>5.608685e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Norway</td>\n",
       "      <td>Norway</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.608685e-07</td>\n",
       "      <td>5.348847e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Norway</td>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>1221.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.348847e+06</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>5.608685e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Norway</td>\n",
       "      <td>Norway</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.608685e-07</td>\n",
       "      <td>5.348847e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Norway</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.348847e+06</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>5.608685e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Norway</td>\n",
       "      <td>Norway</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.608685e-07</td>\n",
       "      <td>5.348847e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Norway</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>1463.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.348847e+06</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>5.608685e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Norway</td>\n",
       "      <td>Norway</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.608685e-07</td>\n",
       "      <td>5.348847e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Norway</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>1550.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.348847e+06</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>1.121737e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Norway</td>\n",
       "      <td>Norway</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.121737e-06</td>\n",
       "      <td>5.348847e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      location_id Province/State Country/Region_x       Date  Confirmed  \\\n",
       "1618           90            NaN           Norway 2020-03-14     1090.0   \n",
       "1619           90            NaN           Norway 2020-03-15     1221.0   \n",
       "1620           90            NaN           Norway 2020-03-16     1333.0   \n",
       "1621           90            NaN           Norway 2020-03-17     1463.0   \n",
       "1622           90            NaN           Norway 2020-03-18     1550.0   \n",
       "\n",
       "      Deaths_x  population_x  Confirmed case rate  Death rate_x  \\\n",
       "1618       3.0  5.348847e+06             0.000204  5.608685e-07   \n",
       "1619       3.0  5.348847e+06             0.000228  5.608685e-07   \n",
       "1620       3.0  5.348847e+06             0.000249  5.608685e-07   \n",
       "1621       3.0  5.348847e+06             0.000274  5.608685e-07   \n",
       "1622       6.0  5.348847e+06             0.000290  1.121737e-06   \n",
       "\n",
       "      Hospitalizations Location Country/Region_y  Days  Deaths_y  \\\n",
       "1618               NaN   Norway           Norway     0       3.0   \n",
       "1619               NaN   Norway           Norway     1       3.0   \n",
       "1620               NaN   Norway           Norway     2       3.0   \n",
       "1621               NaN   Norway           Norway     3       3.0   \n",
       "1622               NaN   Norway           Norway     4       6.0   \n",
       "\n",
       "      Death rate_y  population_y  \n",
       "1618  5.608685e-07  5.348847e+06  \n",
       "1619  5.608685e-07  5.348847e+06  \n",
       "1620  5.608685e-07  5.348847e+06  \n",
       "1621  5.608685e-07  5.348847e+06  \n",
       "1622  1.121737e-06  5.348847e+06  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = checkpoint.load('full_data')\n",
    "death_df = checkpoint.load('deaths')\n",
    "\n",
    "test_df = full_df.merge(death_df, on=['location_id','Date'])\n",
    "test_df[test_df['location_id']==90].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location_id\n",
       "8        0.000006\n",
       "10       0.000003\n",
       "11       0.000009\n",
       "12       0.000002\n",
       "13       0.000048\n",
       "           ...   \n",
       "60391    0.000344\n",
       "60392    0.000343\n",
       "60412    0.003618\n",
       "60886    0.001017\n",
       "60887    0.000431\n",
       "Name: Confirmed case rate, Length: 398, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = cases_and_backcast_deaths_df\n",
    "df = df[df['location_id'] > 0]\n",
    "df = df[df['Confirmed case rate'] > 0]\n",
    "t = df.groupby('location_id')['Confirmed case rate'].mean().dropna()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>Location</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>80</td>\n",
       "      <td>France</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   location_id Location Country/Region  level\n",
       "9           80   France         France      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_run_locs = cases_and_backcast_deaths_df[cases_and_backcast_deaths_df['Deaths'].notnull() & cases_and_backcast_deaths_df['Confirmed case rate'].notnull()]\n",
    "cases_and_backcast_deaths_df[cases_and_backcast_deaths_df['location_id'] == 80].tail()\n",
    "\n",
    "df = cases_and_backcast_deaths_df\n",
    "df = df[df['location_id'] > 0]\n",
    "t = df.groupby(df['location_id']).mean().dropna()\n",
    "model_run_locs = t.index.values\n",
    "\n",
    "model_run_locs = pd.DataFrame(model_run_locs, columns = ['location_id'])\n",
    "model_run_locs.head()\n",
    "\n",
    "loc_df[loc_df['location_id']==80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2134defb4daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdate_mean_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{OUTPUT_DIR}/date_mean_df.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "date_mean_df = runner.make_date_mean_df(threshold_dates)\n",
    "last_day_df = runner.make_last_day_df(smoothed_death_df,date_mean_df)\n",
    "last_day_df.to_csv(f'{OUTPUT_DIR}/last_day.csv', index=False)\n",
    "\n",
    "# this seems to be where some states are lost.\n",
    "# smoothed_death_df.to_csv(f'{OUTPUT_DIR}/smoothed_death_df.csv', index=False)\n",
    "date_mean_df.to_csv(f'{OUTPUT_DIR}/date_mean_df.csv', index=False)\n",
    "\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
